{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSrsNVzuwYzd"
      },
      "source": [
        "# Projet Deep Learning: reconnaissance de cartes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDLRCH_UwYzf"
      },
      "source": [
        "## Chargement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ82vrEOwYzg"
      },
      "source": [
        "### Création du DataFrame et des 3 ensembles train, test et validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PaBuj7cwYzh",
        "outputId": "1d556e2c-e464-4d8e-94c6-bdbf58a4f7e8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colab = True\n",
        "\n",
        "if colab:\n",
        "  drive.mount(\"/content/drive\")\n",
        "  folder = \"/content/drive/MyDrive/dataset-cartes/classes\"\n",
        "else:\n",
        "  folder = \"dataset/classes\"\n",
        "\n",
        "card_type = [\"clubs\", \"diamonds\", \"hearts\", \"spades\"]\n",
        "card_type_to_idx = {card_type[i]: i for i in range(len(card_type))}\n",
        "card_number = [\"seven\", \"eight\", \"nine\", \"ten\", \"jack\", \"queen\", \"king\", \"ace\"]\n",
        "card_number_to_idx = {card_number[i]: i for i in range(len(card_number))}\n",
        "labels = [f\"{n}_{t}\" for n in card_number for t in card_type]\n",
        "labels_to_idx = {labels[i]: i for i in range(len(labels))}\n",
        "# labels_to_idx = {labels[i].split(\"_\")[0]: i for i in range(len(labels))}\n",
        "\n",
        "image_size = 128\n",
        "num_classes = 32\n",
        "num_value = 8\n",
        "num_color = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9ZK-j2XwYzi"
      },
      "outputs": [],
      "source": [
        "def get_dataframe_from_folder(folder, classes):\n",
        "    \"\"\"\n",
        "    Create a dataframe containing information about the dataset from a folder.\n",
        "    Parameters\n",
        "    ----------\n",
        "    folder: string\n",
        "      path to the root folder\n",
        "    classes: []string\n",
        "      names of the classes\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(columns=[\"path\", \"number\", \"type\", \"label\"])\n",
        "    for label in classes:\n",
        "        path_folder_label = os.path.join(folder, label)\n",
        "        for img in os.listdir(path_folder_label):\n",
        "            path = os.path.join(folder, label, img)\n",
        "            s = label.split(\"_\")\n",
        "            card_number = s[0]\n",
        "            card_type = s[1]\n",
        "            df.loc[len(df)] = [path, card_number, card_type, label]\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf2xwylI4w-c"
      },
      "outputs": [],
      "source": [
        "def extract_number_each_class(df, classes, n=70):\n",
        "  \"\"\"\n",
        "  Extract a number of data for each classes.\n",
        "  Parameters\n",
        "  ----------\n",
        "  df: DataFrame\n",
        "    data\n",
        "  classes: []string\n",
        "    names of the classes\n",
        "  n: int, optional\n",
        "    number of data extract from each class\n",
        "  \"\"\"\n",
        "  new_df = pd.DataFrame(columns=[\"path\", \"number\", \"type\", \"label\"])\n",
        "  m = {c: 0 for c in classes}\n",
        "  # for label in classes:\n",
        "  #   for _, row in df.iterrows():\n",
        "  #     if m[row[\"label\"]] < 40:\n",
        "  #       new_df.loc[len(new_df)] = row\n",
        "  for label in classes:\n",
        "    for _, row in df.iterrows():\n",
        "        if m[label] < n and row[\"label\"] == label:\n",
        "            new_df.loc[len(new_df)] = row\n",
        "            m[label] += 1\n",
        "  return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYgNrGeuwYzj"
      },
      "outputs": [],
      "source": [
        "def print_stat(df):\n",
        "    \"\"\"\n",
        "    Print the total number of cards and the number of cards for each label.\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: DataFrame\n",
        "      data\n",
        "    \"\"\"\n",
        "    print(f\"Il y a {len(df)} cartes.\")\n",
        "    sorted_df = df.groupby('label').count().sort_values('path', ascending=False)[\"path\"]\n",
        "    print(sorted_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQIJx5wzwYzk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "def split_train_test_validation(df, ratio=[0.70, 0.15, 0.15], seed=10):\n",
        "    \"\"\"\n",
        "    Split the dataframe in 3 dataframe (train, test and validation)\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: DataFrame\n",
        "      data\n",
        "    ratio: []int, optional\n",
        "      ratio of data for train, test and validation dataset\n",
        "    seed: int, optional\n",
        "      seed for the random state\n",
        "    \"\"\"\n",
        "    n = len(df)\n",
        "    df = df.sample(n=n, random_state=seed)\n",
        "    last_train = math.floor(n * ratio[0])\n",
        "    last_test = last_train + 1 + math.floor(n * ratio[1])\n",
        "    df_train = df[0:last_train]\n",
        "    df_test = df[last_train:last_test]\n",
        "    df_val = df[last_test:]\n",
        "    n_train = len(df_train)\n",
        "    n_test = len(df_test)\n",
        "    n_val = len(df_val)\n",
        "    print(f\"train: {n_train}, test: {n_test}, val: {n_val}, tot: {n_train + n_test + n_val}\")\n",
        "    return df_train, df_test, df_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk-PaFWQwYzk"
      },
      "source": [
        "### Chargement des images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJfUsqxGwYzl"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_data(df, labels_to_idx, image_size=128):\n",
        "    \"\"\"\n",
        "    Split the dataframe in 3 dataframe (train, test and validation)\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: DataFrame\n",
        "      data\n",
        "    labels_to_idx: map[string]int\n",
        "      map a class name to a index\n",
        "    image_size: int, optional\n",
        "      image size\n",
        "    \"\"\"\n",
        "    n = len(df)\n",
        "    x = np.zeros((n, image_size, image_size, 3))\n",
        "    y = np.zeros((n, 1))\n",
        "\n",
        "    index = 0\n",
        "    for _, row in tqdm(df.iterrows(), total=n):\n",
        "        img = Image.open(row[\"path\"])\n",
        "        img = img.convert('RGB')\n",
        "        img = img.resize((image_size, image_size))\n",
        "        x[index] = np.asarray(img)\n",
        "        y[index] = labels_to_idx[row[\"label\"]]\n",
        "        # y[index] = labels_to_idx[row[\"label\"].split(\"_\")[0]]\n",
        "        index += 1\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYfc6Fb1u-zj"
      },
      "source": [
        "### Génération ou chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8OC2jERu-B1"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "generate_dataset = False\n",
        "\n",
        "if generate_dataset:\n",
        "\n",
        "  df_raw = get_dataframe_from_folder(folder, labels)\n",
        "  df_raw.head()\n",
        "  n = 70\n",
        "  df = extract_number_each_class(df_raw,labels,n)\n",
        "  print(\"# df_raw:\")\n",
        "  print_stat(df_raw)\n",
        "  print(\"\\n# df:\")\n",
        "  print_stat(df)\n",
        "\n",
        "  df_train, df_test, df_val = split_train_test_validation(df)#, seed=random.randrange(1000))\n",
        "  x_train, y_train = load_data(df_train, labels_to_idx)\n",
        "  print(x_train.shape, y_train.shape)\n",
        "\n",
        "  x_test, y_test = load_data(df_test, labels_to_idx)\n",
        "  print(x_test.shape, y_test.shape)\n",
        "\n",
        "  x_val, y_val = load_data(df_val, labels_to_idx)\n",
        "  print(x_val.shape, y_val.shape)\n",
        "\n",
        "  hf = h5py.File(folder + \"/dataset.h5\", 'a')\n",
        "  hf.create_dataset('x_train', data=x_train)\n",
        "  hf.create_dataset('y_train', data=y_train)\n",
        "  hf.create_dataset('x_test', data=x_test)\n",
        "  hf.create_dataset('y_test', data=y_test)\n",
        "  hf.create_dataset('x_val', data=x_val)\n",
        "  hf.create_dataset('y_val', data=y_val)\n",
        "  hf.close()\n",
        "  df_train.to_hdf(folder + \"/dataset.h5\", key='df_train', mode='a')\n",
        "  df_test.to_hdf(folder + \"/dataset.h5\", key='df_test', mode='a')\n",
        "  df_val.to_hdf(folder + \"/dataset.h5\", key='df_val', mode='a')\n",
        "else:\n",
        "  hf = h5py.File(folder + \"/dataset.h5\", 'r')\n",
        "  x_train = hf.get('x_train')[:]\n",
        "  y_train = hf.get('y_train')[:]\n",
        "  x_test = hf.get('x_test')[:]\n",
        "  y_test = hf.get('y_test')[:]\n",
        "  x_val = hf.get('x_val')[:]\n",
        "  y_val = hf.get('y_val')[:]\n",
        "  hf.close()\n",
        "  df_train = pd.read_hdf(folder + \"/dataset.h5\", 'df_train')\n",
        "  df_test = pd.read_hdf(folder + \"/dataset.h5\", 'df_test')\n",
        "  df_val = pd.read_hdf(folder + \"/dataset.h5\", 'df_val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL-EaAUTCdkc"
      },
      "source": [
        "### Augmentation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "LwToORueE9JC",
        "outputId": "0ef8dad1-4889-458b-bde2-85b31913c834"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Déclarer un pipeline d'augmentation\n",
        "transforms = A.Compose([\n",
        "    # Transformation en noir et blanc\n",
        "    A.ToGray(p = 0.6),\n",
        "    # Ajouter du CoarseDropout\n",
        "\n",
        "    #max_holes : Le nombre maximum de trous à créer dans l'image. Un trou est une zone masquée.\n",
        "    #max_height : La hauteur maximale d'un trou.\n",
        "    #max_width : La largeur maximale d'un trou.\n",
        "    #min_holes : Le nombre minimum de trous à créer dans l'image.\n",
        "    #min_height : La hauteur minimale d'un trou.\n",
        "    #min_width : La largeur minimale d'un trou.\n",
        "    #fill_value : La valeur utilisée pour remplir les trous.\n",
        "    #p : La probabilité d'appliquer cette transformation à une image donnée.\n",
        "\n",
        "    A.CoarseDropout(max_holes=10, max_height=30, max_width=30, min_holes=2, min_height=5, min_width=5, fill_value=0, p=0.8),\n",
        "\n",
        "    # Autres transformations selon vos besoins\n",
        "    A.ChannelShuffle(p=0.5)\n",
        "    #A.HorizontalFlip(p=0.5),\n",
        "])\n",
        "\n",
        "def apply_custom_transform(image):\n",
        "    return transforms(image=image)['image']\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=apply_custom_transform,\n",
        "\n",
        "    # Ajoutez d'autres paramètres d'augmentation si nécessaire\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "# Utilisez le même générateur pour la validation si vous le souhaitez\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "# Exemple d'utilisation du générateur de données\n",
        "example_x, example_y = train_datagen.flow(x_train, y_train, batch_size=1).next()\n",
        "plt.imshow(example_x[0])  # Afficher l'image en noir et blanc\n",
        "plt.title(labels[int(example_y[0])])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "iLmkj4sfCfzL",
        "outputId": "e149eb6a-122e-41a3-9dde-cd495b38b828"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from PIL import ImageEnhance, Image\n",
        "\n",
        "\n",
        "# Initialiser les générateurs de données avec la fonction de transformation personnalisée\n",
        "train_datagen = ImageDataGenerator(\n",
        "\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "# Exemple d'utilisation du générateur de données\n",
        "example_x, example_y = train_datagen.flow(x_train, y_train, batch_size=1).next()\n",
        "plt.imshow(example_x[0])\n",
        "plt.title(labels[int(example_y[0])])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC6XOmdavNEQ"
      },
      "outputs": [],
      "source": [
        "#train_datagen.fit(x_train)\n",
        "\n",
        "#x_train_gen, y_train_gen = train_datagen.flow(x_train, y_train, batch_size=1)\n",
        "#print(x_train_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyzHbicowYzp"
      },
      "source": [
        "### Visualisation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7b9afXTRwYzp",
        "outputId": "94411b2a-a380-4191-d9f4-bcd838feee33"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_imgs(x, y, labels):\n",
        "    \"\"\"\n",
        "    Display 9 images.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: numpy.array()\n",
        "      images\n",
        "    y: numpy.array()\n",
        "      labels of images\n",
        "    labels: []string\n",
        "      names of labels\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12,12))\n",
        "    shuffle_indices = np.random.permutation(x.shape[0])\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        img = x[shuffle_indices[i]]\n",
        "        plt.title(labels[int(y[shuffle_indices[i]])])\n",
        "        plt.imshow(img/255)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_imgs(x_train, y_train, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYyLs13QMbB5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
        "import itertools\n",
        "\n",
        "def show_imgs_prediction(model, x, y, labels):\n",
        "    \"\"\"\n",
        "    Display prediction of 9 random images with a model.\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: keras model\n",
        "    x: numpy.array()\n",
        "      images\n",
        "    y: numpy.array()\n",
        "      labels of images\n",
        "    labels: []string\n",
        "      names of labels\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12,12))\n",
        "    shuffle_indices = np.random.permutation(x.shape[0])\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        img = x[shuffle_indices[i]]\n",
        "        pred = model.predict(np.array([img,]))\n",
        "        label_pred = np.argmax(pred)\n",
        "        print(label_pred)\n",
        "        plt.title(f\"predict: {labels[label_pred]}, label:{labels[int(y[shuffle_indices[i]])]}\")\n",
        "        plt.imshow(img/255)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_history(history):\n",
        "  \"\"\"\n",
        "  Plot accuracy and loss.\n",
        "  Parameters\n",
        "  ----------\n",
        "  history: keras history\n",
        "    the history\n",
        "  \"\"\"\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs, acc, 'b', linestyle=\"--\",label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'g', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'b', linestyle=\"--\",label='Training loss')\n",
        "  plt.plot(epochs, val_loss,'g', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def confusion_matrix(y, y_pred, labels, figsize=(10,10), text_size=8):\n",
        "  print(len(np.unique(y)))\n",
        "  print(len(np.unique(y_pred)))\n",
        "  cm = sk_confusion_matrix(y, y_pred)\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  n = len(labels)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  color = ax.matshow(cm, cmap=plt.cm.Blues)\n",
        "  fig.colorbar(color)\n",
        "\n",
        "  arange = np.arange(n)\n",
        "  labels = [f\"{l} ({i})\" for i, l in enumerate(labels)]\n",
        "  ax.set(title=\"Confusion matrix\", xlabel=\"Pred\", ylabel=\"True\",\n",
        "        xticks=arange, yticks=arange, xticklabels=arange, yticklabels=labels)\n",
        "\n",
        "  ax.xaxis.set_label_position(\"bottom\")\n",
        "  ax.xaxis.tick_bottom()\n",
        "\n",
        "  threshold = (cm.max() + cm.min()) / 2\n",
        "\n",
        "  for i, j in itertools.product(range(n), range(n)):\n",
        "      if i < cm.shape[0] and j < cm.shape[1]:\n",
        "        v = cm[i,j]\n",
        "      else:\n",
        "        v = 0\n",
        "      plt.text(j, i, f\"{v}\",\n",
        "                horizontalalignment=\"center\",\n",
        "                verticalalignment=\"center\",\n",
        "                color=\"white\" if v > threshold else \"black\",\n",
        "                size=text_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF3YqF14lfda"
      },
      "source": [
        "### Fonction pour split la couleur et la valeur des labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwwazmycle9P"
      },
      "outputs": [],
      "source": [
        "def split_y(y):\n",
        "  \"\"\"\n",
        "  Split 32 classes labels into 8 and 4 classes labels.\n",
        "  Parameters\n",
        "  ----------\n",
        "  y: numpy.array()\n",
        "    labels of images\n",
        "  \"\"\"\n",
        "  y_split = np.zeros((len(y),2))\n",
        "  for i in range(len(y)):\n",
        "    y_split[i,0] = (y[i] // 4)[0]\n",
        "    y_split[i,1] = (y[i] % 4)[0]\n",
        "  return y_split\n",
        "\n",
        "def get_accuracy_split(y, y_pred_value, y_pred_color):\n",
        "  \"\"\"\n",
        "  Compute accuracy on the predicted labels when outputs is splited.\n",
        "  Parameters\n",
        "  ----------\n",
        "  y: numpy.array()\n",
        "    predicted albels\n",
        "  y_pred_value: numpy.array()\n",
        "    value labels of images\n",
        "  y_pred_color: numpy.array()\n",
        "    color labels of images\n",
        "  \"\"\"\n",
        "  y = y.astype(int)\n",
        "  acc_color = 0\n",
        "  acc_value = 0\n",
        "  acc = 0\n",
        "  for i in range(len(y)):\n",
        "    y_value = y[i,0]\n",
        "    y_color = y[i,1]\n",
        "    pred_value = np.argmax(y_pred_value[i])\n",
        "    pred_color = np.argmax(y_pred_color[i])\n",
        "    if y_value == pred_value:\n",
        "      acc_value += 1\n",
        "    if y_color == pred_color:\n",
        "      acc_color += 1\n",
        "    if y_value == pred_value and y_color == pred_color:\n",
        "      acc += 1\n",
        "  print(f\"acc_value = {acc_value/len(y):2f}, acc_color = {acc_color/len(y):2f}, acc = {acc/len(y):2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZUYMSpWVUCj"
      },
      "source": [
        "## Modèle et entrainement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDwVSESvSZHF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Input\n",
        "from keras import regularizers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3SXmmc9NfCS"
      },
      "source": [
        "#### Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quv2PLwXNeYg"
      },
      "outputs": [],
      "source": [
        "## Callbacks\n",
        "checkpoint = ModelCheckpoint('best_model.h5',verbose=1,save_best_only=True,save_weights_only=True)\n",
        "earlystopping = EarlyStopping(patience=10,verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CsZ6RbOTbgT"
      },
      "source": [
        "#### One Hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdBNW9zNRioh"
      },
      "outputs": [],
      "source": [
        "y_train_onehot = to_categorical(y_train, num_classes)\n",
        "y_test_onehot = to_categorical(y_test,num_classes)\n",
        "y_val_onehot = to_categorical(y_val, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAiYNrj2N8cj"
      },
      "outputs": [],
      "source": [
        "y_train_split = split_y(y_train)\n",
        "y_test_split = split_y(y_test)\n",
        "y_val_split = split_y(y_val)\n",
        "y_train_split_value = y_train_split[:,0]\n",
        "y_train_split_color = y_train_split[:,1]\n",
        "y_test_split_value = y_test_split[:,0]\n",
        "y_test_split_color = y_test_split[:,1]\n",
        "y_val_split_value = y_val_split[:,0]\n",
        "y_val_split_color = y_val_split[:,1]\n",
        "y_train_split_onehot_value = to_categorical(y_train_split[:,0], num_value)\n",
        "y_train_split_onehot_color = to_categorical(y_train_split[:,1], num_color)\n",
        "y_test_split_onehot_value = to_categorical(y_test_split[:,0], num_value)\n",
        "y_test_split_onehot_color = to_categorical(y_test_split[:,1], num_color)\n",
        "y_val_split_onehot_value = to_categorical(y_val_split[:,0], num_value)\n",
        "y_val_split_onehot_color = to_categorical(y_val_split[:,1], num_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffgJSNjqzKNw"
      },
      "source": [
        "### Modèle simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXbx_sf9UP-9"
      },
      "source": [
        "#### Entrainement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2Ztzj1GwYzp"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3)))\n",
        "model.add(Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3)))\n",
        "model.add(MaxPool2D(pool_size=2))\n",
        "model.add(Conv2D(filters=64, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3)))\n",
        "model.add(Conv2D(filters=64, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3)))\n",
        "model.add(MaxPool2D(pool_size=2))\n",
        "model.add(Conv2D(filters=128, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3)))\n",
        "model.add(Conv2D(filters=128, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3)))\n",
        "model.add(MaxPool2D(pool_size=3))\n",
        "model.add(Flatten())\n",
        "#model.add(Dense(1024, activation=\"relu\"))\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dense(256, activation=\"relu\"))\n",
        "model.add(Dense(32, activation=\"softmax\"))\n",
        "#model.add(Dense(512, activation=\"relu\",kernel_regularizer=regularizers.L1L2(l1=0.01, l2=0.02)))\n",
        "#model.add(Dense(256, activation=\"relu\",kernel_regularizer=regularizers.L1L2(l1=0.01, l2=0.02)))\n",
        "#model.add(Dense(32, activation=\"softmax\",kernel_regularizer=regularizers.L1L2(l1=0.01, l2=0.02)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUH0FKji1qpq"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=optimizers.Adam(learning_rate=3e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOB-tDE-d7mR",
        "outputId": "675a3955-963d-4bfb-b914-c5b17d0ef85e"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpNtUL9N2DAO",
        "outputId": "13f1d927-8efd-41d7-baad-60bc69beac6f"
      },
      "outputs": [],
      "source": [
        "# Sans augmentation:\n",
        "# history = model.fit(\n",
        "#     x_train/255,\n",
        "#     y_train_onehot,\n",
        "#     validation_data=(x_val/255, y_val_onehot),\n",
        "#     epochs=30,\n",
        "#     batch_size=32\n",
        "# )\n",
        "\n",
        "# Avec augmentation:\n",
        "history = model.fit(train_datagen.flow(x_train, y_train_onehot, batch_size=32),\n",
        "                    validation_data=val_datagen.flow((x_val, y_val_onehot)),\n",
        "                    epochs=70, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "3jfGZ2DvsDL2",
        "outputId": "f0e32e75-fbc4-4519-c3b3-22adebad0250"
      },
      "outputs": [],
      "source": [
        "!pip install visualkeras\n",
        "# from keras.utils import plot_model\n",
        "import visualkeras\n",
        "\n",
        "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, rankdir=\"LR\")\n",
        "\n",
        "visualkeras.layered_view(model).show() # display using your system viewer\n",
        "# visualkeras.layered_view(model, to_file='output.png') # write to disk\n",
        "# visualkeras.layered_view(model, to_file='output.png').show() # write and show\n",
        "\n",
        "visualkeras.layered_view(model, legend=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZJWkkuS8TKK"
      },
      "source": [
        "#### Visualisation de nos résultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BA8rMZMRpcw"
      },
      "source": [
        "##### Prediction et evaluation de l'ensemble de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTbdkvc87th3",
        "outputId": "52f6f14e-9d11-4092-f0f9-bd8f53d7db16"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_test/255, y_test_onehot)\n",
        "y_pred = np.argmax(model.predict(x_test/255), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTRplVxB655-",
        "outputId": "f4fd33d3-3110-49b6-8623-3842d90ee897"
      },
      "outputs": [],
      "source": [
        "# accuracy par classe\n",
        "liste_bonnes_classifs = [0 for i in range(32)]\n",
        "liste_nb_classifs = [0 for i in range(32)]\n",
        "for i in range(len(y_pred)):\n",
        "  if y_pred[i] == y_test[i,0]:\n",
        "    liste_bonnes_classifs[int(y_test[i,0])] = liste_bonnes_classifs[int(y_test[i,0])] + 1\n",
        "  liste_nb_classifs[int(y_test[i,0])] = liste_nb_classifs[int(y_test[i,0])] + 1\n",
        "liste_accuracy = [liste_bonnes_classifs[i]/liste_nb_classifs[i] for i in range(32)]\n",
        "for i in range(32):\n",
        "  print(labels[i].split(\"_\")[0] + \"\\_\" + labels[i].split(\"_\")[1] + \" : \" + str(round(liste_accuracy[i]*100,2)) + \" \\% \" + \"- \" + str(liste_nb_classifs[i]) + \" images de test \\\\\\\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "E7PZCuxr_jaB",
        "outputId": "921b52a5-d8ab-49ce-c529-9ed103e3d3a3"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y8J6feHM-OkT",
        "outputId": "c61e8b37-1d8d-4da4-c3a5-7075c3464cda"
      },
      "outputs": [],
      "source": [
        "show_imgs_prediction(model, x_test, y_test, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N1itp1aLNHN"
      },
      "source": [
        "##### Matrice de confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "rrfFUKW2NiZF",
        "outputId": "641d6758-abef-4147-f1ad-a7ddd4980162"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test, y_pred, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFGP8vl_uMsK"
      },
      "source": [
        "### Modèle simple avec 2 sorties (couleur et valeur)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK-PM_cWuMsK"
      },
      "outputs": [],
      "source": [
        "input_layer = Input(shape=(image_size, image_size, 3))\n",
        "conv2d = Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3))(input_layer)\n",
        "conv2d = Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3))(conv2d)\n",
        "maxpool2d = MaxPool2D(pool_size=2)(conv2d)\n",
        "conv2d = Conv2D(filters=64, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3))(maxpool2d)\n",
        "conv2d = Conv2D(filters=64, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3))(conv2d)\n",
        "maxpool2d = MaxPool2D(pool_size=2)(conv2d)\n",
        "conv2d = Conv2D(filters=128, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3))(maxpool2d)\n",
        "conv2d = Conv2D(filters=128, kernel_size=3, activation=\"relu\", input_shape=(image_size,image_size,3))(conv2d)\n",
        "maxpool2d = MaxPool2D(pool_size=2)(conv2d)\n",
        "flatten = Flatten()(maxpool2d)\n",
        "#modelv2.add(Dense(1024, activation=\"relu\"))\n",
        "dense = Dense(512, activation=\"relu\")(flatten)\n",
        "dense = Dense(256, activation=\"relu\")(dense)\n",
        "\n",
        "output_value = Dense(8, activation=\"softmax\", name=\"value\")(dense)\n",
        "output_color = Dense(4, activation=\"softmax\", name=\"color\")(dense)\n",
        "\n",
        "output = [output_value, output_color]\n",
        "modelv2 = Model(input_layer, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xz0Hg4HuMsK",
        "outputId": "4fe79fc8-1f72-408a-b1ed-486263da761f"
      },
      "outputs": [],
      "source": [
        "modelv2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9GDiT0xuMsL"
      },
      "source": [
        "#### Entrainement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdZUDgAkokJM"
      },
      "outputs": [],
      "source": [
        "loss = [\"categorical_crossentropy\", \"categorical_crossentropy\"]\n",
        "metrics = [[\"accuracy\"], [\"accuracy\"]]\n",
        "loss_weights = [2/3, 1/3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgWWE4Crv8Ks",
        "outputId": "de7d8d0d-bb9f-4424-caf6-4a7abd6a542a"
      },
      "outputs": [],
      "source": [
        "modelv2.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
        "              loss=loss,\n",
        "              metrics=metrics,\n",
        "              loss_weights=loss_weights)\n",
        "\n",
        "\n",
        "\n",
        "historyv2 = modelv2.fit(\n",
        "    x_train,\n",
        "    [y_train_split_onehot_value, y_train_split_onehot_color],\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_data=(x_val, [y_val_split_onehot_value, y_val_split_onehot_color])\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-Ygf6ZYuMsL"
      },
      "source": [
        "#### Visualisation de nos résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cqCk7ebfVhx"
      },
      "outputs": [],
      "source": [
        "def plot_historyv2(history):\n",
        "    \"\"\"\n",
        "    Plot accuracy and loss for a model with two outputs.\n",
        "    Parameters\n",
        "    ----------\n",
        "    history: keras history\n",
        "        the history object returned by model.fit\n",
        "    \"\"\"\n",
        "    # Accuracy and loss for the first output (value)\n",
        "    acc_value = history.history['value_accuracy']\n",
        "    val_acc_value = history.history['val_value_accuracy']\n",
        "    loss_value = history.history['value_loss']\n",
        "    val_loss_value = history.history['val_value_loss']\n",
        "\n",
        "    # Accuracy and loss for the second output (color)\n",
        "    acc_color = history.history['color_accuracy']\n",
        "    val_acc_color = history.history['val_color_accuracy']\n",
        "    loss_color = history.history['color_loss']\n",
        "    val_loss_color = history.history['val_color_loss']\n",
        "\n",
        "    epochs = range(len(acc_value))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot accuracy for value\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, acc_value, 'b', linestyle=\"--\", label='Training value acc')\n",
        "    plt.plot(epochs, val_acc_value, 'g', label='Validation value acc')\n",
        "    plt.title('Training and validation value accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy for color\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, acc_color, 'b', linestyle=\"--\", label='Training color acc')\n",
        "    plt.plot(epochs, val_acc_color, 'g', label='Validation color acc')\n",
        "    plt.title('Training and validation color accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss for value\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, loss_value, 'b', linestyle=\"--\", label='Training value loss')\n",
        "    plt.plot(epochs, val_loss_value, 'g', label='Validation value loss')\n",
        "    plt.title('Training and validation value loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss for color\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, loss_color, 'b', linestyle=\"--\", label='Training color loss')\n",
        "    plt.plot(epochs, val_loss_color, 'g', label='Validation color loss')\n",
        "    plt.title('Training and validation color loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwHD9dTd0EZI",
        "outputId": "a5eea8be-83e5-423e-e996-b269388e5f2b"
      },
      "outputs": [],
      "source": [
        "y_pred_value, y_pred_color = modelv2.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFpg4y8k4k3p",
        "outputId": "89b9fc0b-8837-4913-c899-0640cd75eafc"
      },
      "outputs": [],
      "source": [
        "get_accuracy_split(y_test_split, y_pred_value, y_pred_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "vIuGMEqDg0Zb",
        "outputId": "d7975477-96e3-41b1-c9a8-136e1bcda7ce"
      },
      "outputs": [],
      "source": [
        "plot_historyv2(historyv2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Iu2ZdkjuMsL"
      },
      "source": [
        "##### Prediction et evaluation de l'ensemble de test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxUirC_puMsM"
      },
      "source": [
        "##### Matrice de confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "uTIcO_mduMsM",
        "outputId": "7f03af95-7266-4c01-bfad-143b6503517f"
      },
      "outputs": [],
      "source": [
        "y_test_value = y_test_split[:,0]\n",
        "y_test_color = y_test_split[:,1]\n",
        "confusion_matrix(y_test_value, np.argmax(y_pred_value, axis=1), card_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "yDd0YeWoDhNn",
        "outputId": "c9b25dfe-d999-4910-ad29-b1720635c70f"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test_color, np.argmax(y_pred_color, axis=1), card_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfAOVXuI-Rvt"
      },
      "source": [
        "### Transfer learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhfT4HnSOJpt",
        "outputId": "5ce4da4d-4dd1-47fc-8f10-496d3c0adbba"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50V2, VGG16, MobileNetV2\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Définir le callback ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1)\n",
        "\n",
        "# Assurez-vous d'ajouter le checkpoint callback aussi si ce n'est pas déjà fait\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(image_size, image_size, 3))\n",
        "\n",
        "conv_base.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JqVndJZW01p"
      },
      "source": [
        "#### Entrainement (fine tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1xUAbm6C_BWU",
        "outputId": "efc97799-38c7-4e19-ad94-50e16f746766"
      },
      "outputs": [],
      "source": [
        "model_translearn = Sequential()\n",
        "model_translearn.add(conv_base)\n",
        "model_translearn.add(Flatten())\n",
        "model_translearn.add(Dense(1024, activation=\"relu\"))\n",
        "model_translearn.add(Dense(32, activation=\"softmax\"))\n",
        "model_translearn.summary()\n",
        "\n",
        "initial_epochs = 20\n",
        "\n",
        "\n",
        "# Compiler le modèle avec un taux d'apprentissage plus élevé\n",
        "model_translearn.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "                         loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "# COMPLETER AVEC LES TENSEURS SUR LESQUELS EFFECTUER L'APPRENTISSAGE\n",
        "\n",
        "\n",
        "history = model_translearn.fit(train_datagen.flow(x_train, y_train_onehot, batch_size=32),\n",
        "                    validation_data=val_datagen.flow((x_val, y_val_onehot)),\n",
        "                    epochs=initial_epochs, callbacks=[checkpoint, reduce_lr])\n",
        "\n",
        "\n",
        "conv_base.trainable = True\n",
        "\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow(x_train, y_train_onehot, batch_size=32)\n",
        "\n",
        "validation_generator =  val_datagen.flow((x_val, y_val_onehot))\n",
        "\n",
        "# Fonction pour dégeler progressivement les couches et réentraîner le modèle\n",
        "def fine_tune_model(model, conv_base, train_generator, validation_generator, initial_epochs, additional_epochs, learning_rate, layers_to_unfreeze):\n",
        "    # Dégeler les couches\n",
        "    conv_base.trainable = True\n",
        "    for layer in conv_base.layers[:-layers_to_unfreeze]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Recompiler le modèle avec un taux d'apprentissage plus bas\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Continuer l'entraînement\n",
        "    history_fine = model.fit(train_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    epochs=initial_epochs + additional_epochs,\n",
        "                    steps_per_epoch=len(x_train) // 32,\n",
        "                    validation_steps=len(x_val) // 32,\n",
        "                    callbacks=[checkpoint, reduce_lr])\n",
        "\n",
        "    return history_fine\n",
        "\n",
        "# Définir les paramètres pour le fine-tuning progressif\n",
        "\n",
        "total_epochs = 50\n",
        "fine_tuning_steps = [\n",
        "    {\"epochs\": 10, \"learning_rate\": 1e-5, \"layers_to_unfreeze\": 50},\n",
        "    {\"epochs\": 10, \"learning_rate\": 1e-6, \"layers_to_unfreeze\": 100},\n",
        "    {\"epochs\": 10, \"learning_rate\": 1e-7, \"layers_to_unfreeze\": len(conv_base.layers)}\n",
        "]\n",
        "\n",
        "# Exécuter le fine-tuning progressif\n",
        "current_epochs = initial_epochs\n",
        "for step in fine_tuning_steps:\n",
        "    history_fine = fine_tune_model(model_translearn, conv_base, train_generator, validation_generator,\n",
        "                                   current_epochs, step[\"epochs\"], step[\"learning_rate\"], step[\"layers_to_unfreeze\"])\n",
        "    current_epochs += step[\"epochs\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwT4fTS2DxJY"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSsqkeg5ArcI"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test, y_pred, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abH56WhkWt5p"
      },
      "source": [
        "### Transfer learning avec 2 outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohyf7RAXW9Wb"
      },
      "source": [
        "#### Entrainement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-a4Mh9uWwtb"
      },
      "outputs": [],
      "source": [
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(image_size, image_size, 3))\n",
        "\n",
        "conv_base.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GfRBD4_XZZj",
        "outputId": "b24fb81c-0187-4177-d765-c6854c06ed72"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "input_layer = Input(shape=(image_size, image_size, 3))\n",
        "x = conv_base(input_layer, training=False)\n",
        "x = Flatten()(x)\n",
        "prev_layer = Dense(512, activation=\"relu\")(x)\n",
        "\n",
        "output_value = Dense(8, activation=\"softmax\", name=\"value\")(prev_layer)\n",
        "output_color = Dense(4, activation=\"softmax\", name=\"color\")(prev_layer)\n",
        "\n",
        "output = [output_value, output_color]\n",
        "model_translearn_split = Model(input_layer, output)\n",
        "\n",
        "model_translearn_split.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwFgFy-eXVdV",
        "outputId": "ca1e6739-21ab-4717-947b-793e9212f5ce"
      },
      "outputs": [],
      "source": [
        "model_translearn_split.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
        "              loss=loss,\n",
        "              metrics=metrics,\n",
        "              loss_weights=loss_weights)\n",
        "\n",
        "#  COMPLETER AVEC LES TENSEURS SUR LESQUELS EFFECTUER L'APPRENTISSAGE\n",
        "# history = model_translearn_split.fit(train_datagen.flow(x_train, [y_train_split_onehot_value, y_train_split_onehot_color], batch_size=128),\n",
        "#                     epochs=50, callbacks=[checkpoint])\n",
        "\n",
        "history = model_translearn_split.fit(\n",
        "    x_train,\n",
        "    [y_train_split_onehot_value, y_train_split_onehot_color],\n",
        "    epochs=50,\n",
        "    validation_data=(x_val, [y_val_split_onehot_value, y_val_split_onehot_color]),\n",
        "    batch_size=128\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNrGUZCTgScu",
        "outputId": "4bacc7d1-ec52-47b9-9e91-904d4467372e"
      },
      "outputs": [],
      "source": [
        "y_pred_value, y_pred_color = model_translearn_split.predict(x_test)\n",
        "get_accuracy_split(y_test_split, y_pred_value, y_pred_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "Mxp28g_ogVfA",
        "outputId": "900467ef-781d-4952-92ff-6980a129c9d8"
      },
      "outputs": [],
      "source": [
        "plot_historyv2(history)\n",
        "y_test_value = y_test_split[:,0]\n",
        "y_test_color = y_test_split[:,1]\n",
        "confusion_matrix(y_test_value, np.argmax(y_pred_value, axis=1), card_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "CiGosmajgVMu",
        "outputId": "e397746f-8755-4952-f62c-8e951f0f1368"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test_color, np.argmax(y_pred_color, axis=1), card_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIwMJpD2w6tp"
      },
      "source": [
        "### Transfer learning avec 2 outputs et fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl4F4HxGo6Mu",
        "outputId": "945caff4-a14b-43d7-f9c0-355945706262"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(image_size, image_size, 3))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "input_layer = Input(shape=(image_size, image_size, 3))\n",
        "x = conv_base(input_layer, training=False)\n",
        "x = Flatten()(x)\n",
        "prev_layer = Dense(512, activation=\"relu\")(x)\n",
        "prev_layer = Dense(256, activation=\"relu\")(x)\n",
        "\n",
        "output_value = Dense(8, activation=\"softmax\", name=\"value\")(prev_layer)\n",
        "output_color = Dense(4, activation=\"softmax\", name=\"color\")(prev_layer)\n",
        "\n",
        "output = [output_value, output_color]\n",
        "model_translearn_split = Model(input_layer, output)\n",
        "\n",
        "model_translearn_split.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8XP5KunRblt",
        "outputId": "c3a44af4-682e-44b4-c827-f6708e00c83c"
      },
      "outputs": [],
      "source": [
        "loss_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkkYCHYEolZx",
        "outputId": "6817eb49-2f5f-4d22-920a-62d856e37c8e"
      },
      "outputs": [],
      "source": [
        "initial_epochs = 20\n",
        "\n",
        "model_translearn_split.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
        "              loss=loss,\n",
        "              metrics=metrics,\n",
        "              loss_weights=loss_weights)\n",
        "\n",
        "history = model_translearn_split.fit(x_train,\n",
        "    [y_train_split_onehot_value, y_train_split_onehot_color],\n",
        "    epochs=initial_epochs,\n",
        "    batch_size=128,\n",
        "    validation_data=(x_val, [y_val_split_onehot_value, y_val_split_onehot_color]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt1N8h4LpKma",
        "outputId": "913d1582-af2e-4500-d180-e13e3f8b9d3e"
      },
      "outputs": [],
      "source": [
        "y_pred_value, y_pred_color = model_translearn_split.predict(x_test)\n",
        "get_accuracy_split(y_test_split, y_pred_value, y_pred_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T12UT-LJmdAU",
        "outputId": "ea6fe09d-725b-4591-eb42-e8c1e4ff1387"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "# Fonction pour dégeler progressivement les couches et réentraîner le modèle\n",
        "def fine_tune_model_split(model, conv_base, x_train, y_train, x_val, y_val, initial_epochs, additional_epochs, learning_rate, layers_to_unfreeze):\n",
        "    # Dégeler les couches\n",
        "    conv_base.trainable = True\n",
        "    for layer in conv_base.layers[:-layers_to_unfreeze]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Recompiler le modèle avec un taux d'apprentissage plus bas\n",
        "    model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
        "                  loss=loss,\n",
        "                  metrics=metrics,\n",
        "                  loss_weights=loss_weights)\n",
        "\n",
        "    # Continuer l'entraînement\n",
        "    history_fine = model.fit(x_train, y_train,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    epochs=initial_epochs + additional_epochs,\n",
        "                    steps_per_epoch=len(x_train) // 32,\n",
        "                    validation_steps=len(x_val) // 32,\n",
        "                    callbacks=[checkpoint, reduce_lr])\n",
        "\n",
        "    return history_fine\n",
        "\n",
        "# Définir les paramètres pour le fine-tuning progressif\n",
        "\n",
        "total_epochs = 50\n",
        "fine_tuning_steps = [\n",
        "    {\"epochs\": 10, \"learning_rate\": 1e-5, \"layers_to_unfreeze\": 50},\n",
        "    {\"epochs\": 10, \"learning_rate\": 1e-6, \"layers_to_unfreeze\": 100},\n",
        "    {\"epochs\": 10, \"learning_rate\": 1e-7, \"layers_to_unfreeze\": len(conv_base.layers)}\n",
        "]\n",
        "# Exécuter le fine-tuning progressif\n",
        "current_epochs = initial_epochs\n",
        "for step in fine_tuning_steps:\n",
        "    history_fine = fine_tune_model_split(model_translearn_split, conv_base, x_train, [y_train_split_onehot_value, y_train_split_onehot_color], x_val, [y_val_split_onehot_value, y_val_split_onehot_color],\n",
        "                                   current_epochs, step[\"epochs\"], step[\"learning_rate\"], step[\"layers_to_unfreeze\"])\n",
        "    current_epochs += step[\"epochs\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fgYUbaEwNhm",
        "outputId": "060c6120-d279-44b0-ad63-4e62794c9dc8"
      },
      "outputs": [],
      "source": [
        "y_pred_value, y_pred_color = model_translearn_split.predict(x_test)\n",
        "get_accuracy_split(y_test_split, y_pred_value, y_pred_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "4l1erkLVwYt9",
        "outputId": "121bdb30-2af9-4f8e-91ee-845e4f4a21e3"
      },
      "outputs": [],
      "source": [
        "y_test_value = y_test_split[:,0]\n",
        "y_test_color = y_test_split[:,1]\n",
        "confusion_matrix(y_test_value, np.argmax(y_pred_value, axis=1), card_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "YJCsbgbeweGN",
        "outputId": "1eddaab9-0a02-4ae2-aab1-18ee4c02d061"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test_color, np.argmax(y_pred_color, axis=1), card_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyDC4j4xxsKg"
      },
      "source": [
        "### Transfer learning avec 2 outputs, augmentation et fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CuZwXrJxsKn",
        "outputId": "8ba07c03-1920-4e26-8a5c-509b4f0a7363"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(image_size, image_size, 3))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "input_layer = Input(shape=(image_size, image_size, 3))\n",
        "x = conv_base(input_layer, training=False)\n",
        "x = Flatten()(x)\n",
        "prev_layer = Dense(1024, activation=\"relu\")(x)\n",
        "prev_layer = Dense(256, activation=\"relu\")(x)\n",
        "\n",
        "output_value = Dense(8, activation=\"softmax\", name=\"value\")(prev_layer)\n",
        "output_color = Dense(4, activation=\"softmax\", name=\"color\")(prev_layer)\n",
        "\n",
        "output = [output_value, output_color]\n",
        "model_translearn_split_aug = Model(input_layer, output)\n",
        "\n",
        "model_translearn_split_aug.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzfW4NJPx9d3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class CardSequence(Sequence):\n",
        "    def __init__(self, x_set, y_value, y_color, batch_size, augmentations):\n",
        "        self.x = x_set\n",
        "        self.y_value = y_value\n",
        "        self.y_color = y_color\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augmentations\n",
        "        self.indices = np.arange(x_set.shape[0])\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def apply_augmentation(self, bx):\n",
        "        batch_x = np.zeros((bx.shape[0], 128, 128, 3))\n",
        "        for i in range(len(bx)):\n",
        "            img = bx[i].astype('float32')\n",
        "            transformed = self.augment(image=img.astype('float32'))\n",
        "            batch_x[i] = transformed['image']\n",
        "        return batch_x\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x = self.x[batch_indices]\n",
        "        batch_y_value = self.y_value[batch_indices]\n",
        "        batch_y_color = self.y_color[batch_indices]\n",
        "\n",
        "        batch_x = self.apply_augmentation(batch_x)\n",
        "        return np.array(batch_x), [batch_y_value, batch_y_color]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "train_gen_split = CardSequence(x_train, y_train_split_onehot_value, y_train_split_onehot_color, 32, transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLV9bahVxsKn",
        "outputId": "4b46d034-e6f7-488a-889d-cd70e5e70cbe"
      },
      "outputs": [],
      "source": [
        "initial_epochs = 20\n",
        "\n",
        "model_translearn_split_aug.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
        "              loss=loss,\n",
        "              metrics=metrics,\n",
        "              loss_weights=loss_weights)\n",
        "\n",
        "history = model_translearn_split_aug.fit(train_gen_split,\n",
        "    epochs=initial_epochs,\n",
        "    batch_size=128,\n",
        "    validation_data=(x_val, [y_val_split_onehot_value, y_val_split_onehot_color]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u57Mso5uxsKn",
        "outputId": "14c35adc-291f-4855-f60b-2c5792d3e0f9"
      },
      "outputs": [],
      "source": [
        "y_pred_value, y_pred_color = model_translearn_split_aug.predict(x_test)\n",
        "get_accuracy_split(y_test_split, y_pred_value, y_pred_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfQgjdoExsKo",
        "outputId": "0c37de85-42b5-4261-a5ba-3a268f622bf6"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "# Fonction pour dégeler progressivement les couches et réentraîner le modèle\n",
        "def fine_tune_model_split(model, conv_base, x_train_generator, y_train, y_val, additional_epochs, learning_rate):\n",
        "    # Dégeler les couches\n",
        "    conv_base.trainable = True\n",
        "\n",
        "    # Recompiler le modèle avec un taux d'apprentissage plus bas\n",
        "    model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
        "                  loss=loss,\n",
        "                  metrics=metrics,\n",
        "                  loss_weights=loss_weights)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Continuer l'entraînement\n",
        "    history_fine = model.fit(x_train_generator,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    epochs=initial_epochs + additional_epochs,\n",
        "                    steps_per_epoch=len(x_train) // 32,\n",
        "                    validation_steps=len(x_val) // 32,\n",
        "                    callbacks=[checkpoint, reduce_lr])\n",
        "\n",
        "    return history_fine\n",
        "\n",
        "# Définir les paramètres pour le fine-tuning progressif\n",
        "\n",
        "total_epochs = 50\n",
        "fine_tuning_steps = [\n",
        "    {\"epochs\": 25, \"learning_rate\": 1e-5, \"layers_to_unfreeze\": 50},\n",
        "    {\"epochs\": 25, \"learning_rate\": 1e-6, \"layers_to_unfreeze\": 250},\n",
        "    {\"epochs\": 25, \"learning_rate\": 1e-7, \"layers_to_unfreeze\": len(conv_base.layers)}\n",
        "]\n",
        "# Exécuter le fine-tuning progressif\n",
        "for step in fine_tuning_steps:\n",
        "    history_fine = fine_tune_model_split(model_translearn_split_aug, conv_base, train_gen_split, x_val, [y_val_split_onehot_value, y_val_split_onehot_color],\n",
        "                                  step[\"epochs\"], step[\"learning_rate\"])\n",
        "    current_epochs += step[\"epochs\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKb25jO8xsKo",
        "outputId": "131e7001-ab19-40cc-8255-48ccbba043fe"
      },
      "outputs": [],
      "source": [
        "y_pred_value, y_pred_color = model_translearn_split_aug.predict(x_test)\n",
        "get_accuracy_split(y_test_split, y_pred_value, y_pred_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "6eipZx5ExsKo",
        "outputId": "121bdb30-2af9-4f8e-91ee-845e4f4a21e3"
      },
      "outputs": [],
      "source": [
        "y_test_value = y_test_split[:,0]\n",
        "y_test_color = y_test_split[:,1]\n",
        "confusion_matrix(y_test_value, np.argmax(y_pred_value, axis=1), card_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "O4BrK6MvxsKo",
        "outputId": "1eddaab9-0a02-4ae2-aab1-18ee4c02d061"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test_color, np.argmax(y_pred_color, axis=1), card_type)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
